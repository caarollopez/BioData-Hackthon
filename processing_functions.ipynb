{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<span style=\"font-family: 'Times New Roman'; font-size: 24px; color: white; font-weight: bold;\">Processing and Data Analysis functions</span>",
   "id": "d18617f9e0e5171e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This notebook contains the functions used for the analysis",
   "id": "521dcc38e92ae83d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<span style=\"font-family: 'Times New Roman'; font-size: 22px; color: white; font-weight: bold;\">0. Import libraries and set parameters</span>",
   "id": "2953b375759230b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:35:28.350941Z",
     "start_time": "2024-11-10T16:35:28.336193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile processing_functions.py \n",
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from openpyxl import load_workbook"
   ],
   "id": "b9821deac9d92fbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<span style=\"font-family: 'Times New Roman'; font-size: 22px; color: white;\">Reading and Loading Data</span>",
   "id": "6a949e8860a836d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:35:32.860494Z",
     "start_time": "2024-11-10T16:35:32.835736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def open_file(file_name, datasets_path, dfs):\n",
    "    file_path = os.path.join(datasets_path, file_name)\n",
    "\n",
    "    # Specify the engine to avoid format detection issues\n",
    "    # load all sheets with pandas.read_excel(sheet_name=None)\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None, engine=\"openpyxl\")\n",
    "\n",
    "    # Save each sheet in `dfs` with a unique name combining the file and sheet name\n",
    "    for sheet_name, data in all_sheets.items():\n",
    "        df_name = f\"{file_name.replace('.xlsx', '')}_{sheet_name}\"\n",
    "        dfs[df_name] = data"
   ],
   "id": "747afecb84b249e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<span style=\"font-family: 'Times New Roman'; font-size: 22px; color: white; font-weight: bold;\">1. Functions for the Exploratory Data Analysis</span>",
   "id": "851807db7fb7668c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:35:36.210109Z",
     "start_time": "2024-11-10T16:35:36.198500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def preprocess_dataset(dataset, columns, display_info=True, check_missing=True, check_duplicates=True, check_outliers=True, show_correlation=True):\n",
    "    \"\"\"\n",
    "    This function processes the given dataset by renaming the columns and displaying relevant information.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (pd.DataFrame): The dataset to process.\n",
    "        columns (list): The list of column names to rename the dataset columns to.\n",
    "        display_info (bool): Whether to display basic info about the dataset (default is True).\n",
    "        check_missing (bool): Whether to check for missing values (default is True).\n",
    "        check_duplicates (bool): Whether to check for duplicate rows (default is True).\n",
    "        check_outliers (bool): Whether to check for outliers in numeric columns (default is True).\n",
    "        show_correlation (bool): Whether to display correlation for numeric columns (default is True).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The processed dataset with renamed columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset.columns = columns\n",
    "    \n",
    "    # Display basic information\n",
    "    if display_info:\n",
    "        print(\"------ Dataset Info ------\")\n",
    "        print(f\"Shape: {dataset.shape}\")\n",
    "        print(f\"Columns: {dataset.columns.tolist()}\")\n",
    "        print(f\"First few rows:\\n{dataset.head()}\")\n",
    "        print(f\"Summary statistics:\\n{dataset.describe()}\")\n",
    "        print(\"--------------------------\")\n",
    "    \n",
    "    # Check for missing values and impute\n",
    "    if check_missing:\n",
    "        missing_values = dataset.isnull().sum()\n",
    "        print(\"\\n------ Missing Values ------\")\n",
    "        print(missing_values[missing_values > 0])  # Only show columns with missing values\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "        # Impute missing values\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype in ['float64', 'int64']:  # For numerical columns\n",
    "                mean_value = dataset[col].mean()\n",
    "                dataset[col].fillna(mean_value, inplace=True)\n",
    "                print(f\"Imputed missing values in '{col}' with mean value: {mean_value}\")\n",
    "            elif dataset[col].dtype == 'object':  # For categorical columns\n",
    "                mode_value = dataset[col].mode()[0]\n",
    "                dataset[col].fillna(mode_value, inplace=True)\n",
    "                print(f\"Imputed missing values in '{col}' with mode value: {mode_value}\")\n",
    "        print(\"\\n------ Missing Values After Imputation ------\")\n",
    "        print(dataset.isnull().sum())\n",
    "        print(\"--------------------------\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    if check_duplicates:\n",
    "        duplicate_rows = dataset.duplicated().sum()\n",
    "        print(f\"\\n------ Duplicate Rows ------\")\n",
    "        print(f\"Duplicate Rows: {duplicate_rows}\")\n",
    "        print(\"--------------------------\")\n",
    "    \n",
    "    # Check for outliers in numeric columns\n",
    "    if check_outliers:\n",
    "        numeric_cols = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "        for col in numeric_cols:\n",
    "            q1 = dataset[col].quantile(0.25)\n",
    "            q3 = dataset[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outliers = dataset[(dataset[col] < lower_bound) | (dataset[col] > upper_bound)]\n",
    "            print(f\"\\n------ Outliers in '{col}' ------\")\n",
    "            print(f\"Outliers: {outliers.shape[0]} rows\")\n",
    "            if outliers.shape[0] > 0:\n",
    "                print(outliers[[col]].head())\n",
    "            print(\"--------------------------\")\n",
    "    \n",
    "    # Show correlation matrix (only for numeric columns)\n",
    "    if show_correlation:\n",
    "        numeric_cols = dataset.select_dtypes(include=['float64', 'int64']).columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            correlation_matrix = dataset[numeric_cols].corr()\n",
    "            print(\"\\n------ Correlation Matrix ------\")\n",
    "            print(correlation_matrix)\n",
    "            print(\"--------------------------\")\n",
    "    \n",
    "    return dataset"
   ],
   "id": "83daa7ec6ced31d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:35:37.558237Z",
     "start_time": "2024-11-10T16:35:37.544961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def load_centrifuga_data(id_centrifuga, dfs):\n",
    "    file_name = f'Centrífuga {id_centrifuga}_Datos'\n",
    "    try:\n",
    "        centrifuga_df = dfs[file_name]\n",
    "        return centrifuga_df\n",
    "    except KeyError:\n",
    "        # Si el archivo no existe, retornar un DataFrame vacío o NaN\n",
    "        return pd.DataFrame()  "
   ],
   "id": "3343d20c1f8ae5ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:35:58.399319Z",
     "start_time": "2024-11-10T16:35:58.384324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def calculate_mean_for_lote(lote, fecha_inicio, fecha_fin, id_centrifuga, dfs):\n",
    "    centrifuga_df = load_centrifuga_data(id_centrifuga, dfs)\n",
    "    if centrifuga_df.empty:\n",
    "        return np.nan  # Si no hay datos, devolver NaN\n",
    "    mask = (pd.to_datetime(centrifuga_df['DateTime']) >= pd.to_datetime(fecha_inicio)) & (pd.to_datetime(centrifuga_df['DateTime']) <= pd.to_datetime(fecha_fin))\n",
    "    filtered_data = centrifuga_df[mask]\n",
    "    return filtered_data[f'{id_centrifuga}_D01916047.PV'].mean()"
   ],
   "id": "b9a95cf9706dce17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:36:05.171852Z",
     "start_time": "2024-11-10T16:36:05.160525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def load_bioreactor_data(id_bioreactor, dfs):\n",
    "    file_name = f'Biorreactor {id_bioreactor}_Datos'\n",
    "    try:\n",
    "        # Attempt to load the DataFrame of the bioreactor\n",
    "        bioreactor_df = dfs[file_name]\n",
    "        return bioreactor_df\n",
    "    except KeyError:\n",
    "        # If the file does not exist, return an empty DataFrame\n",
    "        return pd.DataFrame()  # Alternatively, return np.nan if preferred"
   ],
   "id": "30f2f3e0fe7c6802",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:36:09.236729Z",
     "start_time": "2024-11-10T16:36:09.226721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "# Function to calculate the mean for the specified lot and bioreactor\n",
    "def calculate_mean_for_lote2(lote, fecha_inicio, fecha_fin, id_bioreactor, dfs):\n",
    "    bioreactor_df = load_bioreactor_data(id_bioreactor, dfs)\n",
    "    \n",
    "    if bioreactor_df.empty:\n",
    "        return np.nan, np.nan, np.nan  # Return NaN for all means if there's no data\n",
    "\n",
    "    # Filter the data based on the specified date range\n",
    "    mask = (pd.to_datetime(bioreactor_df['DateTime']) >= pd.to_datetime(fecha_inicio)) & \\\n",
    "           (pd.to_datetime(bioreactor_df['DateTime']) <= pd.to_datetime(fecha_fin))\n",
    "    filtered_data = bioreactor_df[mask]\n",
    "    \n",
    "    # Calculate means for the specified columns\n",
    "    mean_temp = filtered_data[f'{id_bioreactor}_FERM0101.Temperatura_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    mean_ph = filtered_data[f'{id_bioreactor}_FERM0101.Single_Use_pH_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    mean_do = filtered_data[f'{id_bioreactor}_FERM0101.Single_Use_DO_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    \n",
    "    return mean_temp, mean_ph, mean_do"
   ],
   "id": "df56370f0680b117",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<span style=\"font-family: 'Times New Roman'; font-size: 22px; color: white; font-weight: bold;\">2. Functions for the Model Analysis</span>",
   "id": "2ea8cc06a3779d3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T16:46:23.475031Z",
     "start_time": "2024-11-10T16:46:23.445271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%writefile -a processing_functions.py \n",
    "def clean_and_convert(column):\n",
    "    # Reemplazar ',' por '.' y eliminar el signo '+'\n",
    "    column = column.str.replace(',', '.', regex=False)\n",
    "    column = column.str.replace('+', '', regex=False)\n",
    "    # Convertir a float, forzando errores a NaN\n",
    "    return pd.to_numeric(column, errors='coerce')"
   ],
   "id": "702c51ab0ea3f572",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to processing_functions.py\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "17fe848441370ae1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
