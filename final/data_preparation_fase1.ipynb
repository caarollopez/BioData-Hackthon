{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:18:58.836762Z",
     "start_time": "2024-11-19T15:18:57.012017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "bdb0ee2e5f7438c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:13.435510Z",
     "start_time": "2024-11-19T15:18:58.843556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = '..\\\\datasets'\n",
    "files = os.listdir(data_path)\n",
    "files.remove('URL descripcion reto.txt')\n",
    "files.remove('leeme.txt')\n",
    "dfs = {}\n",
    "\n",
    "def open_file(file_name):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "\n",
    "    # Cargar todas las hojas con pandas.read_excel(sheet_name=None)\n",
    "    all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "    # Guardar cada hoja en dfs con un nombre único combinando el archivo y la hoja\n",
    "    for sheet_name, data in all_sheets.items():\n",
    "        # Crear un nombre único para el DataFrame usando el nombre del archivo y la hoja\n",
    "        df_name = f\"{file_name.replace('.xlsx', '')}_{sheet_name}\"\n",
    "        dfs[df_name] = data\n",
    "\n",
    "\n",
    "# Abrir todos los archivos y leer todas las hojas\n",
    "for file in files:\n",
    "    if file.endswith('.xlsx'):  # Asegurarse de que solo se procesen archivos Excel\n",
    "        open_file(file)\n"
   ],
   "id": "a32ed3145c20841",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:13.581397Z",
     "start_time": "2024-11-19T15:22:13.565547Z"
    }
   },
   "cell_type": "code",
   "source": "print(dfs.keys())",
   "id": "523baf1bcf5260a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Biorreactor 13169_WData', 'Biorreactor 13169_Datos', 'Biorreactor 13170_WData', 'Biorreactor 13170_Datos', 'Biorreactor 13171_WData', 'Biorreactor 13171_Datos', 'Biorreactor 13172_WData', 'Biorreactor 13172_Datos', 'Biorreactor 14614_Datos', 'Biorreactor 14615_WData', 'Biorreactor 14615_Datos', 'Biorreactor 14616_WData', 'Biorreactor 14616_Datos', 'Biorreactor 14617_WData', 'Biorreactor 14617_Datos', 'Biorreactor 14618_WData', 'Biorreactor 14618_Datos', 'Centrífuga 12912_WData', 'Centrífuga 12912_Datos', 'Centrífuga 14246_WData', 'Centrífuga 14246_Datos', 'Centrífuga 17825_WData', 'Centrífuga 17825_Datos', 'Cinéticos IPC_Inóculos', 'Cinéticos IPC_Cultivos finales', 'Cinéticos IPC_Centrifugación', 'Fases producción v02_Preinóculo', 'Fases producción v02_Inóculo', 'Fases producción v02_Cultivo final', 'Fases producción v03 Test_Cultivo final', 'Fases producción v03_Preinóculo', 'Fases producción v03_Inóculo', 'Fases producción v03_Cultivo final', 'Fases producción_test v02_Cultivo final', 'Horas inicio fin centrífugas_Hoja1', 'Movimientos componentes_Full1', 'OF 123456 v02_Sheet1', 'OF 123456 v03_Sheet1', 'Temperaturas y humedades_WData', 'Temperaturas y humedades_Datos'])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:13.645170Z",
     "start_time": "2024-11-19T15:22:13.613200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Acceder a los DataFrames\n",
    "\n",
    "df_preinoculo = dfs['Fases producción v03_Preinóculo']\n",
    "df_preinoculo.columns = df_preinoculo.iloc[0]\n",
    "df_preinoculo = df_preinoculo.drop(0)\n",
    "df_inoculo = dfs['Fases producción v03_Inóculo']\n",
    "df_cultivo_final = dfs['Fases producción v03_Cultivo final']\n",
    "\n",
    "merged_df = df_preinoculo.merge(df_inoculo, on='LOTE', suffixes=('_preinoculo', '_inoculo'))\n",
    "merged_df = merged_df.merge(df_cultivo_final, on='LOTE', suffixes=('', '_cultivo_final'))"
   ],
   "id": "35a8e96a5de4515f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:13.697344Z",
     "start_time": "2024-11-19T15:22:13.689833Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "920096a7f6d11e5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:24.019670Z",
     "start_time": "2024-11-19T15:22:13.744421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_centrifuga_data(id_centrifuga):\n",
    "    file_name = f'Centrífuga {id_centrifuga}_Datos'\n",
    "    try:\n",
    "        # Intentar cargar el DataFrame de centrífuga\n",
    "        centrifuga_df = dfs[file_name]\n",
    "        return centrifuga_df\n",
    "    except KeyError:\n",
    "        # Si el archivo no existe, retornar un DataFrame vacío o NaN\n",
    "        return pd.DataFrame()  # o simplemente return np.nan si prefieres\n",
    "\n",
    "\n",
    "def calculate_mean_for_lote(lote, fecha_inicio, fecha_fin, id_centrifuga):\n",
    "    centrifuga_df = load_centrifuga_data(id_centrifuga)\n",
    "    if centrifuga_df.empty:\n",
    "        return np.nan  # Si no hay datos, devolver NaN\n",
    "    mask = (pd.to_datetime(centrifuga_df['DateTime']) >= pd.to_datetime(fecha_inicio)) & (pd.to_datetime(centrifuga_df['DateTime']) <= pd.to_datetime(fecha_fin))\n",
    "    filtered_data = centrifuga_df[mask]\n",
    "    return filtered_data[f'{id_centrifuga}_D01916047.PV'].mean()\n",
    "\n",
    "merged_df['media_PV'] = merged_df.apply(\n",
    "    lambda row: calculate_mean_for_lote(row['LOTE'], row['Fecha/hora inicio'], row['Fecha/hora fin'], row['ID Centrífuga']),\n",
    "    axis=1\n",
    ")"
   ],
   "id": "545962fadc22575e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:34.233644Z",
     "start_time": "2024-11-19T15:22:24.066665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Function to load bioreactor data based on the given ID\n",
    "def load_bioreactor_data(id_bioreactor):\n",
    "    file_name = f'Biorreactor {id_bioreactor}_Datos'\n",
    "    try:\n",
    "        # Attempt to load the DataFrame of the bioreactor\n",
    "        bioreactor_df = dfs[file_name]\n",
    "        return bioreactor_df\n",
    "    except KeyError:\n",
    "        # If the file does not exist, return an empty DataFrame\n",
    "        return pd.DataFrame()  # Alternatively, return np.nan if preferred\n",
    "\n",
    "# Function to calculate the mean for the specified lot and bioreactor\n",
    "def calculate_mean_for_lote2(lote, fecha_inicio, fecha_fin, id_bioreactor):\n",
    "    bioreactor_df = load_bioreactor_data(id_bioreactor)\n",
    "    \n",
    "    if bioreactor_df.empty:\n",
    "        return np.nan, np.nan, np.nan  # Return NaN for all means if there's no data\n",
    "\n",
    "    # Filter the data based on the specified date range\n",
    "    mask = (pd.to_datetime(bioreactor_df['DateTime']) >= pd.to_datetime(fecha_inicio)) & \\\n",
    "           (pd.to_datetime(bioreactor_df['DateTime']) <= pd.to_datetime(fecha_fin))\n",
    "    filtered_data = bioreactor_df[mask]\n",
    "    \n",
    "    # Calculate means for the specified columns\n",
    "    mean_temp = filtered_data[f'{id_bioreactor}_FERM0101.Temperatura_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    mean_ph = filtered_data[f'{id_bioreactor}_FERM0101.Single_Use_pH_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    mean_do = filtered_data[f'{id_bioreactor}_FERM0101.Single_Use_DO_PV'].mean() if not filtered_data.empty else np.nan\n",
    "    \n",
    "    return mean_temp, mean_ph, mean_do\n",
    "\n",
    "# Apply the function to each row of merged_df and store the results in new columns\n",
    "merged_df[['media_temp_bioreactor', 'media_ph_biorreactor', 'media_PO_biorreactor']] = merged_df.apply(\n",
    "    lambda row: calculate_mean_for_lote2(row['LOTE'], row['Fecha/hora inicio_inoculo'], row['Fecha/hora fin_inoculo'], row['ID bioreactor']),\n",
    "    axis=1,\n",
    "    result_type='expand'  # This allows unpacking into multiple columns\n",
    ")"
   ],
   "id": "18da64027993a890",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:22:34.297195Z",
     "start_time": "2024-11-19T15:22:34.265372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df = merged_df.fillna(\"NA\")\n",
    "output_file_path = 'dataset_fase1.csv'  # Cambia esto a la ruta donde quieras guardar el archivo\n",
    "merged_df.to_csv(output_file_path, index=False)"
   ],
   "id": "85cd8ecc922e7a32",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:18:44.391828100Z",
     "start_time": "2024-11-10T13:39:17.247414Z"
    }
   },
   "cell_type": "code",
   "source": "merged_df.c",
   "id": "1b5fa9670079870d",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'c'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmerged_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mc\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5895\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5896\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   5897\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   5898\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   5899\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   5900\u001B[0m ):\n\u001B[0;32m   5901\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 5902\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'c'"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
